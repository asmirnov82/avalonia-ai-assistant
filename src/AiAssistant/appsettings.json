{
    "InferenceParams": {
        "SystemInstructions": "You're an intelligent, concise coding assistant. Wrap code in ``` for readability. Don't repeat yourself. Use best practice and good coding standards.",
        "Temperature": "0.5",
        "PresencePenalty": "0",
        "FrequencyPenalty": "0"
    },

    //Phi 3 Mini Instruct
    /*
    "ModelParams": {
        "FileName": "Phi-3-mini-4k-instruct-Q4_K_M.gguf",
        "Path": "C:\\ML\\Models\\Phi-3-mini-4k\\",
        "GpuLayerCount": "33",
        "TotalLayerCount": "33",
        "ContextSize": "4096"
    }
    */

    //Phi 3 Medium
    /*
    "ModelParams": {
        "FileName": "Phi-3-medium-4k-instruct-Q4_K_M.gguf",
        "Path": "C:\\ML\\Models\\Phi-3-medium-4k\\",
        "GpuLayerCount": "18",
        "TotalLayerCount": "41",
        "ContextSize": "4096"
    }
    */

    //Lama2 7B Chat
    "ModelParams": {
        "FileName": "llama-2-7b-chat.Q4_K_M.gguf",
        "Path": "C:\\ML\\Models\\LLaMA\\",
        "GpuLayerCount": "33",
        "TotalLayerCount": "33",
        "ContextSize": "4096",
        "CustomHistoryTransformer": "Llama2"
    }

    /*
    //Lama3 8B Instruct
    "ModelParams": {
        "FileName": "llama-3-8B-Instruct.Q4_K_M.gguf",
        "Path": "C:\\ML\\Models\\LLaMA\\",
        "GpuLayerCount": "24",
        "TotalLayerCount": "33",
        "ContextSize": "8192"
    }
    */
}
